# -*- coding: utf-8 -*-
"""GAE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yI7rEhFGSwrQlGMPyeyseZrFnz5dGXa_
"""


import torch
import torch.nn as nn
from torch.distributions import MultivariateNormal
import gym
import numpy as np
from IPython.display import clear_output
import matplotlib.pyplot as plt
import torch.nn.functional as F
import pybullet_envs

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

def plot(avg_rewards, value_losses, action_losses, log_interval):
    '''        For monitoring the training process    '''
    clear_output(True)
    plt.figure(figsize=(7,7)) 
    plt.title(' reward: %s' % (np.mean(avg_rewards[-10:])))
    plt.plot(avg_rewards)
    plt.show()

    
    plt.figure(figsize=(17,5))
    plt.subplot(1,3,1)
    plt.title('Episodic reward: %s' % (np.mean(avg_rewards[-log_interval:])))
    plt.scatter(np.linspace(0,len(avg_rewards)-1,len(avg_rewards)),avg_rewards, s=1)

    plt.subplot(1,3,2)
    plt.title('value loss: %s' % ( np.mean(value_losses[-log_interval:])))
    plt.scatter(np.linspace(0,len(value_losses)-1,len(value_losses)),value_losses, s=1)
    
    plt.subplot(1,3,3)
    plt.title('action loss: %s' % ( np.mean(action_losses[-log_interval:])))
    plt.scatter(np.linspace(0,len(action_losses)-1,len(action_losses)),action_losses, s=1)
    plt.show()
    
    
    
    



class Memory:
    def __init__(self):
        self.actions = []
        self.states = []
        self.next_states = []
        self.logprobs = []
        self.rewards = []
        self.dones = []
        self.bootstrapes = []
    
    def clear_memory(self):
        del self.actions[:]
        del self.states[:]
        del self.logprobs[:]
        del self.rewards[:]
        del self.dones[:]
        del self.bootstrapes[:]
        del self.next_states[:]
        
    def insert(self, state,action,next_state,reward,done,bootstrap):
        self.states.append(state)
        self.actions.append(action)
        self.next_states.append(next_state)
        self.rewards.append(reward)
        self.dones.append(done)
        self.bootstrapes.append(bootstrap)
        

class Critic(nn.Module):
    def __init__(self, state_dim):
        super(Critic, self).__init__()
        '''To Do: Create an Actor network with 2 hidden layers.
                  Each hidden layer with 64 neurons and using PReLU() as activation.
                  The output of the network is the approximated value estimate for the state'''
        self.critic = nn.Sequential(
             nn.Linear(in_features=state_dim, out_features=64),
             nn.PReLU(),
             nn.Linear(in_features=64, out_features=64),
             nn.PReLU(),
             nn.Linear(in_features=64, out_features=64),
             nn.PReLU(),             
             nn.Linear(in_features=64, out_features=1)#?
             )

    def forward(self,state):
        return self.critic(state)
        
    def evaluate_state_value_no_gradient(self, state):
        with torch.no_grad():
            value = self.forward(state).detach()
        return value
    
    
    def evaluate_state_value(self, state):
        value = self.forward(state)            
        return value

    
    
class Actor(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(Actor, self).__init__()
        self.da = action_dim
        self.mean_layer = nn.Linear(64, self.da)
        #self.mean_layer_act = nn.Tanh()
        self.cholesky_layer = nn.Linear(64, (self.da * (self.da + 1)) // 2)    
        '''To Do: Create an Actor network with 2 hidden layers.
                  Each hidden layer with 64 neurons and using PReLU() as activation.
                  The output of the network should be:
                  (a) the mean for each action dimension :mu
                  (b) CHANGED : We now assume dependence between each action, i.e., off-diagonal element of covariance matrix is no longer zero
                  Since the off-diagonal element is symmetric, we use Cholesky decomposition to reduce the number of parameters.
                  Cov = cholesky*cholesky^T , you could refer to that online.               
                  The output of Cov matrix is therefore (action_dim)*(action_dim+1)/2  
                  Refer to :  https://github.com/daisatojp/mpo/blob/master/mpo/actor.py for an example.
        '''
        self.actor =  nn.Sequential(
             nn.Linear(in_features=state_dim, out_features=64),
             nn.PReLU(),
             nn.Linear(in_features=64, out_features=64),
             nn.PReLU(),
             nn.Linear(in_features=64, out_features=64),
             nn.PReLU(),
             #nn.Linear(in_features=64, out_features=2*action_dim)                         
             )         
        
  

        
    def forward(self, state):
        ''' Input : state (Tensor)
            Return : mu and Cholesky  (Tensor)
            Cholesky dim : (Batch, self.da, self.da)
        '''
        
        B = state.size(0)
        #action_low = torch.from_numpy(self.env.action_space.low)[None, ...].to(device)  # (1, da)
        #action_high = torch.from_numpy(self.env.action_space.high)[None, ...].to(device)  # (1, da)
        mu = self.mean_layer(self.actor(state))
        #mean = torch.sigmoid(mu)  # (B, da)
        #mean = action_low + (action_high - action_low) * mean
        # To do: Diagonal element in Cholesky must be a positive number, therefore, Use F.softplus() to var to make it strictly positive
        #cholesky = F.softplus(self.cholesky_layer(self.actor(state))) # a lower triangular matrix.
        cholesky_vector = self.cholesky_layer(self.actor(state)) # (B, (da*(da+1))//2)
        cholesky_diag_index = torch.arange(self.da, dtype=torch.long) + 1
        cholesky_diag_index = (cholesky_diag_index * (cholesky_diag_index + 1)) // 2 - 1
        cholesky_vector[:, cholesky_diag_index] = F.softplus(cholesky_vector[:, cholesky_diag_index])
        tril_indices = torch.tril_indices(row=self.da, col=self.da, offset=0)
        cholesky = torch.zeros(size=(B, self.da, self.da), dtype=torch.float32).to(device)
        cholesky[:, tril_indices[0], tril_indices[1]] = cholesky_vector
        #print(cholesky)
        return mu, cholesky
        
    def act(self, state):
        ''' To Do:
            Refer to https://pytorch.org/docs/stable/distributions.html
            (1) first pass the state S to NN to get action_mean, and action_var
            (2) Create a torch.distributions.MultivariateNormal which takes the mean and variance, variance stays the same for all states.
            (3) sample an action according to the normal distribution in step 2 ,  torch.distributions.sample() , 
            Return action (in tensor), Note : .to(device) is required to make sure tensors are all on GPU or CPU.
            useful function:  MultivariateNormal(), .sample() 
            Input : state  (Tensor, [1, state_dim])
            Return: action (Tensor, [1, action_dim])
        '''
        with torch.no_grad():
            action_mean, cholesky = self.forward(state)
        # loc=loc, scale_tril=scale_tril
        dist =  MultivariateNormal(action_mean,scale_tril=cholesky) # initiate an instance of  multivariate Gaussian distribution. NOTE: here we use cholesky decomposition , 'scale_tril' should be set as cholesky.
        action = dist.sample() # sample an action from dist, which is of shape [1, action_dim]
        return action.detach()

    def act_test(self, state):
        ''' For testing phase, we would like to have deterministic action 
            by taking the mean of the Gaussian Distribution.
            Return test_action (in tensor)
            Input : state  (Tensor, [1, state_dim])
            Return: action (Tensor, [1, action_dim])
        '''       
        with torch.no_grad():
            action_mean,_ = self.forward(state)
        return action_mean.detach()
    
    def evaluate(self, state, action):   
        ''' To Do: Compute the action_logprobs and entropy in batch mode (for all samples in memory)        
            Useful function in pytorch :  MultivariateNormal(), distribution.log_prob(), distribution.entropy()  
            see https://pytorch.org/docs/stable/distributions.html            
            Input : state (Tensor, [#rollout, state_dim]), action (Tensor, [#rollout,action_dim])
            Return: action_logprobs (1-D tensor)--> torch.size([rollout]) , dist_entropy (1-D tensor)--> torch.size([rollout])
            The first dimension of cov_mat takes the form of an diagonalized matrix [action_dim_1_var, 0...
                                                                                     0, action_dim_2_var, 0,...,
                                                                                     ...
                                                                                     0 ......, action_dim_N_var], 
            where off-diagonal elements are 0, assuming independence among each action dimension.
        '''  
        action_mean, cholesky= self.forward(state)        
        dist = MultivariateNormal(loc=action_mean,scale_tril=cholesky)     # NOTE: here we use cholesky decomposition , 'scale_tril' should be set as cholesky.
        if len(action.shape) == 1:
            action = action.unsqueeze(1)
        # Refer to : https://github.com/pranz24/pytorch-soft-actor-critic/issues/18 so that a correct gradient to Actor can be formulated. 
        # This is needed because we squash the action betwenn [-1,1], which is not the original action proposed by the network.
        # useful function : torch.log(), torch.tanh(),...
        action_logprobs = dist.log_prob(action)
        action_logprobs=(action_logprobs - torch.log(1 - (torch.tanh(action)).pow(2)).sum(1, keepdim=True))
        
        dist_entropy = dist.entropy()  
        return action_logprobs , dist_entropy



class A2C:
    def __init__(self, state_dim, action_dim, lr, betas, gamma):
        self.lr = lr
        self.betas = betas
        self.gamma = gamma
        self.actor = Actor(state_dim, action_dim).to(device)
        self.critic = Critic(state_dim).to(device)   
        self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr=lr)
        self.optimizer_critic = torch.optim.Adam(self.critic.parameters(), lr=lr*2)
        self.MseLoss = nn.MSELoss()
    
    def select_action(self, state):
        state = torch.FloatTensor(state.reshape(1, -1)).to(device)
        return self.actor.act(state).cpu().data.numpy().flatten()


    def select_test_action(self, state):
        state = torch.FloatTensor(state.reshape(1, -1)).to(device)
        return self.actor.act_test(state).cpu().data.numpy().flatten()
    
        
    
    def update(self, memory):
        '''(i) Implement the Lambda returns using GAE here.
           (ii) WE update the critic multiple times. To be specific, we update the critic for a first time, and then re-evalaute the V(S) and its target lmbda-return with GAE,
           and then perform a second update. This iteration goes on until 20 cycles are reached. Important is the re-evlauation of V(s) and regression target in each cycle.'''
        # convert list first to np.array and then to tensor, to increase the efficiency, which is much efficient than directly converting list to tensor.
        states = np.array(memory.states)
        states = torch.FloatTensor(states).to(device)
        actions = np.array(memory.actions)
        actions = torch.squeeze(torch.FloatTensor(np.stack(actions)).to(device), 1)
        #
        
        # --------------------compute value loss----------------
        # IMPORTANT: Compute the Monte-Carlo Returns as the update target for all V(S_t)        
        state_values = self.critic.evaluate_state_value(states)         
        sv=state_values.detach().cpu().numpy()
        #print(sv.shape)
        # IMPORTANT: we will compute advantage based on the returns, the advantage we use here is the fourth formulae in Page 22 (the one with no bias and high variance) in http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-6.pdf.
        returns = [] # Target value for V(S_t), where Monte-Carlo returns are used.
 
        next_states = np.array(memory.next_states)
        next_states = torch.FloatTensor(next_states).to(device) 
        next_state_values = self.critic.evaluate_state_value_no_gradient(next_states).squeeze().cpu().numpy()
        # To Do: initialize the td_target with last element of next_state_values to allow for bootstrapping in case of pseudo termination(max episodic length reached)
        td_target = next_state_values[-1]
        # ----------Start computing Lambda returns Using Generalized Advantage Estimation (GAE)------------
        ct = -1
        gae = 0
        for reward, done, bootstrap in zip(reversed(memory.rewards), reversed(memory.dones), reversed(memory.bootstrapes)):
            # returns = [td_target(S_0),td_target(S_1)...,td_target(S_T-1), # for the first episode
            #            td_target(S_0),td_target(S_1)...,td_target(S_T-1), # for the second episode
            #            ,....,
            #            td_target(S_0),td_target(S_1)...,td_target(S_T-1), # for the last episode]
            # returns is computed in reversed order!! First compute td_target(S_T-1) from the last episode,
            # then to  td_target(S_T-2) from the last epi,....td(S_0) of the last epi, td(S_T-1) of the LAST SECOND epi, td(S_T-2).... until the first
            #if done==True and bootstrap==False: # 
            #    td_target = 0 # To think
            #elif bootstrap==True: # Pseudo termination due to max episodic length being reached
            #    td_target = next_state_values[ct] # to think,   
            td_target = reward+(self.gamma*td_target*(1-done)) #.... a recursive form to compute the td_target as the return, 'td_target' should appear on the right part of the equation.
            td_delta = td_target-state_values[ct] # a function of td_target ?lambda
            gae = td_delta+self.gamma*(1-done)*(1-bootstrap)*0.95*gae   # A function of gae and td_delta
            # To do: Insert the td_target into the first element of the list 'returns', while maintaning the existing elements in the list.
            returns.insert(0,td_target) # Hint: shouldn't be the same as before.
            ct -= 1
            
        returns = torch.FloatTensor(returns).to(device)
        
        #------------------ update action loss---------------- 
        # To Do: compute advantages
        #print(returns.shape)
        #print(state_values.shape)
        advantages=returns-state_values.detach()# TO do... ,as mentioned above we Monte Carlo returns to compute advantages, See http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-6.pdf ,       
        logprobs, dist_entropy = self.actor.evaluate(states, actions)        
        action_loss = (-logprobs*advantages).mean() # To Do :  
        # Encourage the agent to explore by maximizing the entropy
        action_loss =  action_loss - 0.01 * dist_entropy.mean() # TO think : plus the entropy or minus the entropy   0.01 * dist_entropy.mean() 
        self.optimizer_actor.zero_grad()
        action_loss.backward()
        self.optimizer_actor.step()       
        # print('value loss: {}, action loss: {}'.format(value_loss, action_loss))  
        
        #-----------Update value estimate-------------
        # TO DO:
        #state_values=state_values.clone().detach().requires_grad_(True)
        for i in range(20):
            state_values = self.critic.evaluate_state_value(states)
            #print(state_values.shape)
            #print(returns.shape)
            #sleep()
            value_loss = 0.5*self.MseLoss(state_values.squeeze(),returns)
            self.optimizer_critic.zero_grad()
            value_loss.backward()
            self.optimizer_critic.step()            

        return value_loss.cpu().data.numpy().flatten(), action_loss.cpu().data.numpy().flatten()


def main():
    ############## Hyperparameters ##############
    env_name = 'LunarLanderContinuous-v2'  
    # creating environment
    env = gym.make(env_name)
    env_test = gym.make(env_name)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    render = False
    solved_reward = np.inf         # stop training if avg_reward > solved_reward
    log_interval = 1           # print avg reward in the interval
    max_iterations = 10000        # max training episodes
    max_timesteps = env._max_episode_steps        # max timesteps in one episode
    print(env._max_episode_steps)
    update_every_N_complete_episode = 8      # IMPORTANT : update policy every _N_complete_episode, for A2C, this is set to be the number of parallel agents.
    gamma = 0.99                # discount factor
    lr = 0.0005                # parameters for Adam optimizer
    betas = (0.9, 0.999)
    #print(betas)
    # Note: Boostraping mode affects (1) how the MC-returns are computed in A2C.update() 
    # (2) If enabled, only in case of the pseudo termination (reaching maximal episode instead of terminal states), done is set as False, Bootstrap flag is set true.
    # For A2C, we set bootstrapping mode to be True
    allow_bootstrapping_mode = True    
    random_seed = None
    #############################################

    
    if random_seed:
        print("Random Seed: {}".format(random_seed))
        torch.manual_seed(random_seed)
        env.seed(random_seed)
        np.random.seed(random_seed)
    
    memory = Memory()
    a2c = A2C(state_dim, action_dim, lr, betas, gamma)
    
    # logging variables
    episodic_reward = 0
    time_step = 0 # Count of episodic length within each update iteration, of different definition from previous versions
    done_count_in_an_itr = 0
    avg_rewards, value_losses, action_losses = [] ,[], []
    
    # training loop, one iteration means collecting (N = 8) complete episodes and then do the update for both actor and critic.
    for i_iter in range(1, max_iterations+1):
        state = env.reset()            
        episodic_reward, avg_reward = 0, 0
        done_count_in_an_itr, time_step = 0 , 0
        # -----------------Testing_phase-----------------
        if i_iter % 10 == 0:                  
            test_reward = 0
            test_state = env_test.reset()            
            test_done = False
            print('-----------starting test-----------')
            for i in range(max_timesteps):
                action = a2c.select_test_action(test_state)
                # env_test.render()
                # To do:squash the action into [-1,1], using np.tanh
                squashed_action = np.tanh(action)
                test_state, reward, test_done, _ = env_test.step(squashed_action)
                test_reward += reward
                if test_done == True:
                    break
            print('Test reward : {}'.format(test_reward))
       
        
        while done_count_in_an_itr < update_every_N_complete_episode:
            time_step +=1
            # Run policy
            action = a2c.select_action(state)
            # To do : squash the action into [-1,1], using np.tanh
            squashed_action = np.tanh(action)
            next_state, reward, done, _ = env.step(squashed_action)

            # Saving reward and dones to the temporary buffer: 
            if (time_step == env._max_episode_steps) and (done == True):     
                if allow_bootstrapping_mode:
                    # Enable bootstrapping for the last experience reaching maximal episodic length
                    memory.insert(state, action, next_state, reward, False, True)
                else:
                    # Disable bootstrapping
                    memory.insert(state, action, next_state, reward, done, False)
                time_step = 0
            else:
                memory.insert(state, action, next_state, reward, done, False)
            
            # update after collecting N complete epsidoes, rollout size can differ for each iteration
            if (done_count_in_an_itr == update_every_N_complete_episode - 1) and (done):
                value_loss, action_loss = a2c.update(memory)
                # Clear the memory when update is done!
                memory.clear_memory()
                value_losses.append(value_loss[0])
                action_losses.append(action_loss[0])
            
            state = next_state    
            episodic_reward += reward
            
            if done:  
                state = env.reset()
                done_count_in_an_itr += 1
                avg_reward += (1/done_count_in_an_itr) * (episodic_reward-avg_reward)
                episodic_reward = 0
                time_step = 0
                avg_rewards.append(avg_reward)

        
        # stop training if avg_reward > solved_reward
        if avg_reward > solved_reward:
            print("########## Solved! ##########")
            # You can save network here for animation purpose
            break
        
            
        # logging
        if i_iter % log_interval == 0:
            print('Iteration {} \t Avg reward: {}'.format(i_iter, avg_reward))
            plot(avg_rewards, value_losses, action_losses, log_interval)
            
if __name__ == '__main__':
    main()
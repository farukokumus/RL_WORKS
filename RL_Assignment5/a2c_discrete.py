# -*- coding: utf-8 -*-
"""A2C_discrete.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16NnAwpQRs8ecMwh3ky5SVCmQomHmvWfR
"""

#!pip install pybullet

import torch
import torch.nn as nn
from torch.distributions import Categorical
import torch.nn.functional as F 
import gym
import numpy as np
import pybullet_envs
import random
from IPython.display import clear_output
import matplotlib.pyplot as plt
import time
get_ipython().run_line_magic('matplotlib', 'inline')

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


def plot(avg_rewards,action_losses,log_interval):
    '''        For monitoring the training process    '''
    clear_output(True)
    plt.figure(figsize=(7,7)) 
    plt.title(' reward: %s' % (np.mean(avg_rewards[-10:])))
    plt.plot(avg_rewards)
    plt.show()

    
    plt.figure(figsize=(11,5))
    plt.subplot(121)
    plt.title('Episodic reward: %s' % (np.mean(avg_rewards[-log_interval:])))
    plt.scatter(np.linspace(0,len(avg_rewards)-1,len(avg_rewards)),avg_rewards, s=1)
    #epi_returns_np = np.array(epi_returns)
    #person="FRK"
    #np.save("REINFORCE" + person + ".npy", (np.mean(avg_rewards[-log_interval:])))

    plt.subplot(122)
    plt.title('Action loss: %s' % ( np.mean(action_losses[-log_interval:])))
    plt.scatter(np.linspace(0,len(action_losses)-1,len(action_losses)),action_losses, s=1)
    plt.show()
    #time.sleep(0.5)

class Memory:
    def __init__(self):
        self.actions = []
        self.states = []
        self.next_states = []
        self.logprobs = []
        self.rewards = []
        self.dones = []
        self.bootstrapes = []
    
    def clear_memory(self):
        del self.actions[:]
        del self.states[:]
        del self.logprobs[:]
        del self.rewards[:]
        del self.dones[:]
        del self.bootstrapes[:]
        del self.next_states[:]
        
    def insert(self, state,action,next_state,reward,done,bootstrap):
        self.states.append(state)
        self.actions.append(action)
        self.next_states.append(next_state)
        self.rewards.append(reward)
        self.dones.append(done)
        self.bootstrapes.append(bootstrap)



    
class Actor(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(Actor, self).__init__()
        '''To Do: Create an Actor network with 2 hidden layers.
        Each hidden layer with 64 neurons and using LeakyReLU(slope = 0.2) as activation. Output layer without activation
        The output of the network should be the number of discrete actions, 
        cooresponding to the probability of each action being chosen.
        Input argument: action_std is NOT USED in the discrete action space.
        '''
                  
        self.actor =  nn.Sequential(
             nn.Linear(in_features=state_dim, out_features=64),
             nn.LeakyReLU(0.2),
             nn.Linear(in_features=64, out_features=64),
             nn.LeakyReLU(0.2),
             nn.Linear(in_features=64, out_features=64),
             nn.LeakyReLU(0.2),
             nn.Linear(in_features=64, out_features=action_dim)
             )

        
    def forward(self):
        raise NotImplementedError
        """
        state=(state.float()/255.).cuda() # cuda yı bir check edelim !! fazladan birdaha cuda yapıyoruz
        feat=self.features(state)   
        feat = feat.view(feat.size(0), -1) # check x.size()[0] birde burada hep x kullanıyoruz o bir problem olabilr
        fc1=self.fc1(feat)
        lrelu=self.leaky_relu(fc1)
        x=self.fc2(lrelu).cuda()
        return x      
        """

    def act(self, state):

        ''' To Do:
        https://pytorch.org/docs/stable/distributions.html
        (1) first pass the state S to NN to get probailities for each, use F.softmax() to make sure the probability of each action is non-negative and the probability of all actions sum up to 1.
        (2) Create a torch.distribution.Categorical, which takes softmax version of probabilties 
        (3) sample an action according to the normal distribution in step 2, torch.distributions.sample()
        Input : state  (Tensor, [1, state_dim])
        Return: action (Tensor, [1])
        Note : .to(device) is required to make sure tensors are all on GPU or CPU.

        '''
        with torch.no_grad():
            action_probs = F.softmax(self.actor(state),dim=-1)# To do:Perform softmax on the output to to get a probability on all actions. shape:[1,n_action]
        action_distribution = Categorical(action_probs) # To do:instantiate a Categorical distribution with action_probs
        action = (action_distribution.sample()).to(device) # To do:sample an action according to action_distribution , shape: [1]
        action = action.detach()
  
        return action

    def act_test(self, state):
        ''' To do: For testing phase, we would like to have deterministic action 
            by taking the action with the largest probability.
            Input : state  (Tensor, [1, state_dim])
            Return: action (Tensor, [1]) 
            Useful function: torch_Categorical_object.probs.argmax()
        '''
        with torch.no_grad():
            action_probs = F.softmax(self.actor(state),dim=-1)# To do: Perform softmax on the output to get a probability on all actions . shape:[1,n_action]
        action_distribution =  Categorical(action_probs)# To do: instantiate a Categorical distribution with action_probs
        #print(action_distribution)
        action = (action_distribution.probs.argmax()).to(device)# To do: Take the action index with the largest prob , shape: [1]
        return action.detach()
    
    def evaluate(self, state, action):   
        ''' To Do: Compute the action_logprobs and entropy in batch mode (for all samples in memory)       
            Useful function in pytorch : distribution.log_prob(), distribution.entropy()  
            see https://pytorch.org/docs/stable/distributions.html            
            Input : state (Tensor, [#rollout, state_dim]), action (Tensor, [#rollout, action_dim])
            Return: action_logprobs (1-D tensor)--> torch.size([rollout]) , dist_entropy (1-D tensor)--> torch.size([rollout])           
        '''  
        
        action_probs = F.softmax(self.actor(state),dim=-1)# as above
        action_distribution = Categorical(action_probs)# as above
        #action=action_distribution.sample()
        action_logprobs = action_distribution.log_prob(action)
        dist_entropy = action_distribution.entropy()
        return action_logprobs, dist_entropy



class REINFORCE:
    def __init__(self, state_dim, action_dim, lr, betas, gamma):
        self.lr = lr
        self.betas = betas
        self.gamma = gamma
        self.actor = Actor(state_dim, action_dim).to(device)  
        self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr=lr)
        self.MseLoss = nn.MSELoss()
    
    def select_action(self, state):
        state = torch.FloatTensor(state.reshape(1, -1)).to(device)
        return self.actor.act(state).cpu().data.numpy().flatten()
    
    def select_test_action(self, state):
        state = torch.FloatTensor(state.reshape(1, -1)).to(device)
        return self.actor.act_test(state).cpu().data.numpy().flatten()
    
    
    def update(self, memory):
        '''This part is mostly similar to A2C-single-thread-continuous.py, where the difference is REINFORCE with Causality update'''
        # convert list to tensor
        states = torch.squeeze(torch.FloatTensor(np.stack(memory.states)).to(device), 1)
        actions = torch.squeeze(torch.FloatTensor(np.stack(memory.actions)).to(device), 1)  
        
        # --------------------compute value loss----------------
        # Compute the Monte-Carlo Returns as the update target for all V(S_t)               
        returns = [] 
      
        # To Do: initialize the td_target 0
        td_target = 0 #np.zeros((states.shape))
        # ----------Start computing returns------------

        for reward, done, bootstrap in zip(reversed(memory.rewards), reversed(memory.dones), reversed(memory.bootstrapes)):
            # Compute the returns according to causality.
            # returns = [td_target(S_0),td_target(S_1)...,td_target(S_T-1), # for the first episode
            #            td_target(S_0),td_target(S_1)...,td_target(S_T-1), # for the second episode
            #            ,....,
            #            td_target(S_0),td_target(S_1)...,td_target(S_T-1), # for the last episode]
            # returns is computed in reversed order!! First compute td_target(S_T-1) from the last episode,
            # then to  td_target(S_T-2) from the last epi,....td(S_0) of the last epi, td(S_T-1) of the LAST SECOND episode, td(S_T-2).... until the first
            if done==True and bootstrap==False: # 
                td_target = 0 # To think           
            td_target = (td_target * self.gamma) + reward  #.... a recursive form to compute the td_target as the return, 'td_target' should appear on the right part of the equation.
            # To do: Insert the td_target into the first element of the list 'returns', while maintaning the existing elements in the list.
            returns.insert(0,td_target)

            
        returns = torch.FloatTensor(returns).to(device)

  
        #------------------ update action loss(REINFORCE)----------------    
        # Compute log-likelihood and entropy from the current distribution
        logprobs, dist_entropy = self.actor.evaluate(states, actions)
        #td_estimates = self.actor.act(states)  # To Do :  

        action_loss = (returns * -logprobs).mean()#self.MseLoss(logprobs,returns)
        # Encourage the agent to explore by maximizing the entropy
        action_loss = action_loss - (0.01 * dist_entropy.mean()) # TO think : plus the entropy or minus the entropy   0.01 * dist_entropy.mean() 
        self.optimizer_actor.zero_grad()
        action_loss.backward()
        self.optimizer_actor.step()       
        # print('value loss: {}, action loss: {}'.format(value_loss, action_loss))  
        return action_loss.cpu().data.numpy().flatten()
        
        
def main():
    # Although you don't need to modify the program in the main(), it is definitely necessary to read into that.
    # 

    ############## Hyperparameters ##############
    env_name = 'CartPoleBulletEnv-v1'
    # creating environment
    env = gym.make(env_name)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n
    render = False
    solved_reward = np.inf         # stop training if avg_reward > solved_reward
    log_interval = 1           # print avg reward in the interval
    max_iterations = 10000        # max training episodes
    max_timesteps = env._max_episode_steps        # max timesteps in one episode
    print(env._max_episode_steps)
    update_every_N_complete_episode = 8      # IMPORTANT:  update policy every _N_complete_episode. For REINFORCE, this should be 1. But for a stable performance, it is recommeded to set as 8.  
    gamma = 0.99                # discount factor
    lr = 0.001                 # parameters for Adam optimizer
    betas = (0.9, 0.999)
    # Note: Boostraping mode affects (1) How the MC-returns are computed in REINFORCE.update() 
    # (2) If enabled, only in case of the pseudo termination (reaching maximal episode instead of terminal states), done is set as False, Bootstrap flag is set true.
    # For REINFORCE, we set bootstrapping mode to be False, as bootstrapping requires a critic approximating the V(S), but REINFORCE by nature doesn't have a critic part.
    allow_bootstrapping_mode = False   
    random_seed = None
    #############################################
    

    
    if random_seed:
        print("Random Seed: {}".format(random_seed))
        torch.manual_seed(random_seed)
        env.seed(random_seed)
        np.random.seed(random_seed)
    
    memory = Memory()
    reinforce = REINFORCE(state_dim, action_dim, lr, betas, gamma)
    print(lr,betas)
    
    # logging variables
    episodic_reward = 0
    avg_length = 0
    time_step = 0 # Count of episodic length within each update iteration, of different definition from previous versions
    done_count_in_an_itr = 0 # For statistical purpose: Counts for (#True termination for an episode + #Maximal episodic length reached)
    avg_rewards, action_losses = [] , []

    # training loop, one iteration means collecting (N = 8) complete episodes and then do the update for actor.
    for i_iter in range(1, max_iterations+1):
        state = env.reset()           
        episodic_reward, avg_reward = 0, 0
        done_count_in_an_itr = 0
        time_step = 0
        
            # -----------------Test----------------
        if i_iter % 10 == 0:                  
            test_reward = 0
            env_test = gym.make(env_name)
            test_state = env_test.reset()
            test_done = False
            print('-----------starting test-----------')
            for i in range(max_timesteps):
                action = reinforce.select_test_action(test_state)
                # env_test.render()
                test_state, reward, test_done, _ = env_test.step(action[0])
                test_reward += reward
                if test_done == True:
                    break
            print('Test reward : {}'.format(test_reward))
            #time.sleep(1)
            # break        
        
        while done_count_in_an_itr < update_every_N_complete_episode:
            time_step +=1
            # Running policy_old:
            action = reinforce.select_action(state)
            next_state, reward, done, _ = env.step(action[0])

            # Saving reward and dones: 
            if (time_step == env._max_episode_steps) and (done == True):
                if allow_bootstrapping_mode:
                    # Enable bootstrapping for the last experience reaching maximal episodic length
                    memory.insert(state, action, next_state, reward, False, True)
                else:
                    # Disable bootstrapping
                    memory.insert(state, action, next_state, reward, done, False)
                time_step = 0
            else:
                memory.insert(state, action, next_state, reward, done, False)
            
            # update if collecting enough experience
            if (done_count_in_an_itr == update_every_N_complete_episode - 1) and done:
                value_loss = reinforce.update(memory)
                memory.clear_memory()
                state = env.reset()
                action_losses.append(value_loss[0])
                    
            episodic_reward += reward
            state = next_state 
            
            if done:  
                state = env.reset()
                done_count_in_an_itr += 1
                avg_reward += (1/done_count_in_an_itr) * (episodic_reward-avg_reward)
                episodic_reward = 0
                time_step = 0
                avg_rewards.append(avg_reward)
            

        # stop training if avg_reward > solved_reward
        if avg_reward > solved_reward:
            print("########## Solved! ##########")
            #-------- You could save the network here------------
            print("Training is done and model is saving!!!")
            torch.save(model.state_dict(),"./")
            break
        
            
        # logging
        if i_iter % log_interval == 0:
            print('Iteration {} \t Avg reward: {}'.format(i_iter, avg_reward))
            plot(avg_rewards, action_losses, log_interval)
            
if __name__ == '__main__':
    main()
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_on_Colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qyo_j5rNbb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27d2b620-2200-4ddf-d056-bd2d2a012ab3"
      },
      "source": [
        "!pip install gym==0.17.1\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install gym[atari]\n",
        "!pip install pyvirtualdisplay\n",
        "!conda install piglet\n",
        "!pip install pystan\n",
        "!conda install swig\n",
        "!pip install box2d-py\n",
        "!pip install gym[Box_2D]\n",
        "!pip install Box2D\n",
        "# !pip3 install pybullet --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym==0.17.1 in /usr/local/lib/python3.7/dist-packages (0.17.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.17.1) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym==0.17.1) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym==0.17.1) (1.15.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.17.1) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.17.1) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.1) (0.16.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.9).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "Looking in links: https://github.com/Kojoley/atari-py/releases\n",
            "Requirement already satisfied: atari_py in /usr/local/lib/python3.7/dist-packages (0.2.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from atari_py) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari_py) (1.15.0)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (2.2)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay) (0.3)\n",
            "/bin/bash: conda: command not found\n",
            "Requirement already satisfied: pystan in /usr/local/lib/python3.7/dist-packages (2.19.1.1)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from pystan) (1.19.5)\n",
            "Requirement already satisfied: Cython!=0.25.1,>=0.22 in /usr/local/lib/python3.7/dist-packages (from pystan) (0.29.23)\n",
            "/bin/bash: conda: command not found\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.19.5)\n",
            "--2021-06-11 14:32:13--  http://www.atarimania.com/roms/Roms.rar\n",
            "Resolving www.atarimania.com (www.atarimania.com)... 195.154.81.199\n",
            "Connecting to www.atarimania.com (www.atarimania.com)|195.154.81.199|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11128004 (11M) [application/x-rar-compressed]\n",
            "Saving to: ‘Roms.rar’\n",
            "\n",
            "Roms.rar            100%[===================>]  10.61M   474KB/s    in 24s     \n",
            "\n",
            "2021-06-11 14:32:37 (460 KB/s) - ‘Roms.rar’ saved [11128004/11128004]\n",
            "\n",
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from Roms.rar\n",
            "\n",
            "Extracting  HC ROMS.zip                                                  \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Extracting  ROMS.zip                                                     \b\b\b\b 74%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n",
            "unzip:  cannot find or open Roms/ROMS.zip, Roms/ROMS.zip.zip or Roms/ROMS.zip.ZIP.\n",
            "Collecting gym-retro\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/f0/d7cc8f49be83c8608d5f707650a917a85e656ea51684aa434a1b6b8024fe/gym_retro-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (162.0MB)\n",
            "\u001b[K     |████████████████████████████████| 162.0MB 37kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyglet==1.*,>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from gym-retro) (1.5.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from gym-retro) (0.17.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet==1.*,>=1.3.2->gym-retro) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym->gym-retro) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym->gym-retro) (1.15.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->gym-retro) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->gym-retro) (1.4.1)\n",
            "Installing collected packages: gym-retro\n",
            "Successfully installed gym-retro-0.8.0\n",
            "Imported 0 games\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcHiNIzAMDyo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "f7de8bb3-77df-4a5b-d3b8-338dc6b0d5a8"
      },
      "source": [
        "import math, random\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd \n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "\n",
        "\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, capacity, state_dim):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "        self.state_dim = state_dim\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        '''Add new samples to replay buffer'''\n",
        "        experience = (state, action, reward, next_state, done)\n",
        "        self.buffer.append(experience)\n",
        "        # record the dimension of the state\n",
        "        \n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        ''' To Do: sample a batch uniformly from replay buffer, sample without replacement.\n",
        "            Return a tuple (state_batch, action_batch, reward_batch, next_state_batch, done_batch), where\n",
        "            state_batch, next_state_batch: 2D numpy array, [batch_size,state_dim]\n",
        "            action_batch, reward_batch, done_batch: 1D numpy array [batch_size]\n",
        "            Note : the order in these arrays must be matched. \n",
        "            i.e. state_batch = [S_1,S_2,S_3,...S_n]\n",
        "                 action_batch = [a_1,a_2,a_3....a_n] , n = batch_size, the same for the rest three batches\n",
        "            Useful function: np.empty() for initializing N-dim array\n",
        "                             random.sample() for sampling without replacement\n",
        "        '''\n",
        "        # first initialize using np.empty()\n",
        "        state_batch = ...\n",
        "        action_batch = ...\n",
        "        reward_batch = ...\n",
        "        next_state_batch = ...\n",
        "        done_batch = ...\n",
        "        # Take #batch_size samples from the replay buffer and write into different arrays.        \n",
        "        batch = random.sample(....)\n",
        "        # To Do here...\n",
        "        ...\n",
        "        \n",
        "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
        "\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, num_inputs, num_actions):\n",
        "        super(DQN, self).__init__()  \n",
        "        ''' To Do: Create the following network architecture:\n",
        "            (1) The first input layer, a fully connected (FC) layer with (input_dim*64), followed by a PReLU layer, see literature for how PRelu works\n",
        "            (2) A hidden layer, also FC, 64*64, and followed by PRELU\n",
        "            (3) The output layer, FC layer, (64*number_of_actions) , no activations, as the output approximates the q(s,a)\n",
        "            Useful function: nn.Sequential() , nn.Linear(), nn.PReLU()\n",
        "        '''\n",
        "        self.layers = #To do...\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        ''' Retrieve the approximated q(s,a) for the given input x=(s,a)\n",
        "        '''\n",
        "        return self.layers(x)\n",
        "    \n",
        "    def act(self, state, epsilon):\n",
        "        ''' To Do: Perform epislon greedy exploration strategy here,\n",
        "            Return : action index in discrete action space, type(action) = int\n",
        "            You will need to call the function: self.forward(state)\n",
        "            Use np.random.random() \n",
        "        '''\n",
        "        return action\n",
        "\n",
        "    def test_act(self, state):\n",
        "        ''' To Do: Perform greedy action for the current state for the testing phase\n",
        "            Return : action index in discrete action space, type(action) = int\n",
        "            You will need to call the function: self.forward(state)\n",
        "\n",
        "        '''\n",
        "        return action\n",
        "\n",
        "\n",
        "def compute_td_loss(batch_size, replay_buffer, optimizer, device, model, model_target, gamma):\n",
        "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
        "    state      = torch.FloatTensor(np.float32(state)).to(device)\n",
        "    next_state = torch.FloatTensor(np.float32(next_state)).to(device)\n",
        "    action     = torch.LongTensor(action).to(device)\n",
        "    reward     = torch.FloatTensor(reward).to(device)\n",
        "    done       = torch.FloatTensor(done).to(device)\n",
        "\n",
        "    '''To Do : Implement the DQN/ DDQN algorithm update rule here \n",
        "    You need to define the loss metric as Mean Square Error. \n",
        "    The difference between DQN & DDQN is shown in these steps.\n",
        "    Useful function: in-place operations on tensors 'tensor_a.gather()' , 'tensor_b.unsqueeze()', 'tensor_x.squeeze()' , 'tensor.max()'\n",
        "    Use : with torch.no_grad() ,  tensor.detach()  to make your td_target have no influence on backward gradient, i.e. semi-gradient on your td_target\n",
        "    '''\n",
        "    loss = ....# Use MSEloss with mean reduction.\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "    return loss\n",
        "\n",
        "\n",
        "def test(env, model):       \n",
        "    ''' \n",
        "        Testing phase.\n",
        "        Only for local users, you can uncomment env.render()\n",
        "    '''\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "    # Limit the maximal episode length to avoid infinite loop\n",
        "    for t in range(1000): \n",
        "        action = model.test_act(state)\n",
        "        next_state, reward, done, _ = env.step(action)    \n",
        "        #env.render()        \n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "        if done == True:                  \n",
        "            print(\"test reward : {}\".format(episode_reward))\n",
        "            break\n",
        "\n",
        "\n",
        "def plot(frame_idx, rewards, losses, task):\n",
        "    '''\n",
        "        For monitoring the training process\n",
        "    '''\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(121)\n",
        "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
        "    if task == 1:\n",
        "        plt.plot(rewards)\n",
        "    elif task == 2:\n",
        "        plt.scatter(np.linspace(0,len(rewards)-1,len(rewards)),rewards, s=0.75)\n",
        "    plt.subplot(122)\n",
        "    plt.title('loss')\n",
        "    plt.plot(losses)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":    \n",
        "    '''\n",
        "        Important: The maximal number of interactions: num_frames = 1000000,\n",
        "        But it is definitely NOT NECESSARY to ran until the end.\n",
        "        If the training episodic reward already converges >-150, then you can STOP the program.\n",
        "        It takes roughly 10 minutes to get converged to episodic reward >-150 in Colab. \n",
        "        You need to write code to save the statistics at the last line of the program.       \n",
        "    '''\n",
        "    task = 1 # to modify for different task\n",
        "    if task == 1:\n",
        "        env = gym.make('MountainCar-v0').env # the suffix .env removes the constraint of maximal episodic length of 200 steps \n",
        "    elif task == 2:\n",
        "        env = gym.make('LunarLander-v2') # An episode will be forced to terminate after 1000 steps in this env setting.\n",
        "    print(env.observation_space, env.action_space)   #observation_space and action space\n",
        "    \n",
        "    \n",
        "    # Initialize the Deep Q-networks\n",
        "    model = DQN(env.observation_space.shape[0], env.action_space.n)\n",
        "    # Declare target network, initialize it \n",
        "    model_target = DQN(env.observation_space.shape[0], env.action_space.n)\n",
        "    \n",
        "    # -->To Do: Copy the weights from model to model_target using 'load_state_dict'\n",
        "    model_target....\n",
        "    \n",
        "    # Put networks to GPU\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model_target = model_target.to(device)\n",
        "    \n",
        "    # Initialize the optimizer for learning the weights of Neural Network\n",
        "    optimizer = optim.Adam(model.parameters(), lr = 0.001)    \n",
        "    if task == 1: \n",
        "        batch_size = 64\n",
        "        test_every_N_training_episode = 10\n",
        "        start_train = 1000\n",
        "        \n",
        "        # Initialize the exploration strategy/coefficient\n",
        "        epsilon_start = 1.0\n",
        "        epsilon_final = 0.05 # to ensure sufficient exploration \n",
        "        epsilon_decay = 30000 \n",
        "        epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)  \n",
        "        # Plot the exploration rate w.r.t. number of steps (frame_index) the agent has traversed. \n",
        "        plt.plot([epsilon_by_frame(i) for i in range(100000)])\n",
        "        \n",
        "    elif task == 2:\n",
        "        batch_size = 64\n",
        "        test_every_N_training_episode = 10\n",
        "        # Only start training when there is a sufficient number of experience stored in replay buffer\n",
        "        start_train = 3000 \n",
        "        \n",
        "        # Initialize the exploration strategy/coefficient\n",
        "        epsilon_start = 1.0\n",
        "        epsilon_final = 0.05 # to ensure sufficient exploration \n",
        "        epsilon_decay = 50000 \n",
        "        epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)  \n",
        "        # Plot the exploration rate w.r.t. number of steps (frame_index) the agent has traversed. \n",
        "        plt.plot([epsilon_by_frame(i) for i in range(100000)])\n",
        "        \n",
        "    # Initialize the replay buffer with the maximal storage of 1000000 experience/interactions\n",
        "    replay_buffer = ReplayBuffer(600000, env.observation_space.shape)    \n",
        "\n",
        "    gamma = 0.99 # discount factor\n",
        "\n",
        "    # Statistics \n",
        "    losses = []\n",
        "    all_rewards = []\n",
        "    episode_reward = 0\n",
        "    episode_count = 0\n",
        "    episodic_step_count = 0\n",
        "    \n",
        "    # -----state_trail is the initial state whose estimated q-value will be examined.----- \n",
        "    state_trial = env.reset()\n",
        "    state_trial = torch.FloatTensor(np.float32(state_trial)).to(device)\n",
        "    est_Q_values_running_network = []\n",
        "    est_Q_values_target_network = []\n",
        "       \n",
        "    #---------------------Training----------------------\n",
        "    # You don't need to run 0.5M frames, you could terminate earlier when the return is around -150. Save the statistics of episodic returns and predicted Q-values\n",
        "    num_frames = 1000000 # maximal number of interactions, similar to N_episodes in previous assignments \n",
        "    \n",
        "    state = env.reset()\n",
        "    for frame_idx in range(1, num_frames + 1):\n",
        "        # determine the exploration rate for this step\n",
        "        epsilon = epsilon_by_frame(frame_idx)\n",
        "        # Take an action according to exploration strategy\n",
        "        action = model.act(state, epsilon)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        episodic_step_count += 1\n",
        "        # Add the experience to the replay buffer\n",
        "        if task == 1:\n",
        "            replay_buffer.push(state, action, reward, next_state, done)       \n",
        "        elif task == 2:\n",
        "            # If terminate due to reaching max epi length, we put done as False to replay buffer so to allow bootstrapping of next successor state\n",
        "            if episodic_step_count == env._max_episode_steps:  \n",
        "                replay_buffer.push(state, action, reward, next_state, False)\n",
        "            else:\n",
        "                replay_buffer.push(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        episode_reward += reward       \n",
        "        \n",
        "        if done:\n",
        "            all_rewards.append(episode_reward)\n",
        "            print(\"Episode#:{} reward:{} eps:{}\".format(episode_count,\n",
        "                                     episode_reward, epsilon))\n",
        "            episode_reward = 0\n",
        "            episode_count += 1\n",
        "            episodic_step_count = 0\n",
        "            # ---------------------Test Phase--------------------\n",
        "            if len(all_rewards)%test_every_N_training_episode == test_every_N_training_episode-1:\n",
        "                test(env, model)\n",
        "            #----------------------------------------------\n",
        "            state = env.reset()\n",
        "            \n",
        "            \n",
        "        # Ensure there are enough samples in the replay buffer, then start training the network    \n",
        "        if len(replay_buffer) > start_train:\n",
        "            loss = compute_td_loss(batch_size, replay_buffer, optimizer, device, model, model_target, gamma)\n",
        "            losses.append(loss.detach().cpu().numpy())\n",
        "            # -------------check the estimated Q-value for initial state on both running network and target network--------------\n",
        "            with torch.no_grad():\n",
        "                est_Q_values_running_network.append(np.max(model(state_trial).cpu().numpy()))\n",
        "                est_Q_values_target_network.append(np.max(model_target(state_trial).cpu().numpy()))\n",
        "         \n",
        "        # update target network                         \n",
        "        if task == 1: \n",
        "            # To Do : Perform soft update (Polyak averaging) with tau = 0.005 \n",
        "            # Soft update # Refer to the post from Navneet_M_Kumar under https://discuss.pytorch.org/t/copying-weights-from-one-net-to-another/1492/16 for answer           \n",
        "            ...\n",
        "        elif task == 2: \n",
        "            # To do: hard update the target network every 3000 interactions. Useful variable: frame_idx\n",
        "            ...    \n",
        "         \n",
        "        if frame_idx % 2500 == 0:\n",
        "            plot(frame_idx, all_rewards, losses, task)\n",
        "            plt.plot(np.array(est_Q_values_running_network), color = 'r')\n",
        "            plt.plot(np.array(est_Q_values_target_network), color = 'b')\n",
        "            # --> To Do: You can save the statistics here, using np.save()\n",
        "            # You could first convert all_rewards and losses into np.array, and save as .npy.file\n",
        "            \n",
        "            \n",
        "            \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-fa3b0a415b61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0menv_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"PongNoFrameskip-v4\"\u001b[0m \u001b[0;31m# you could change to other games, but only use 'NoFrameskip-v4' version. Some games e.g., spaceinvaders require changing the code in wrapper, i.e., frame_skip =3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_atari\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_deepmind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_pytorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Note the env returns the unnormalized pixel [0,255].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-c35f40d2310c>\u001b[0m in \u001b[0;36mmake_atari\u001b[0;34m(env_id)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m#----------------Original-----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Making new env: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# reset/step. Set _gym_disable_underscore_compat = True on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Make the enviroment aware of which spec it came from.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, game, mode, difficulty, obs_type, frameskip, repeat_action_probability, full_action_space)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matari_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_game_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_difficulty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdifficulty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/atari_py/games.py\u001b[0m in \u001b[0;36mget_game_path\u001b[0;34m(game_name)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_games_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".bin\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ROM is missing for %s, see https://github.com/openai/atari-py#roms for instructions'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgame_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: ROM is missing for pong, see https://github.com/openai/atari-py#roms for instructions"
          ]
        }
      ]
    }
  ]
}
# -*- coding: utf-8 -*-
"""atari_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16muRMZJzQOs3CpDv6lxI5IffRs_aOIOD
"""

##-----degraded-----##
# !pip install -Iv gym==0.17.1
# !apt-get install python-opengl -y
# !apt install xvfb -y
# !pip install gym[atari]
# !pip install pyvirtualdisplay
# !conda install piglet
# !pip install pystan
# !conda install swig
# #!pip install box2d-py
# !pip install box2d-py
# !pip install gym[box2d]

# Upload 'HC ROMS.zip' and 'ROMS.zip' online.
!python -m atari_py.import_roms .
import gym
env = gym.make('Pong-v0')
print(env.observation_space.shape)

# -*- coding: utf-8 -*-
"""
Created on Thu Jun 10 18:00:43 2021
Don't chnage the relative position of files.
@author: hongh
"""

import math, random
import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.autograd as autograd 
import torch.nn.functional as F
import itertools
from collections import deque
from common.old_atari_wrappers import wrap_pytorch, make_atari, wrap_deepmind
import matplotlib.pyplot as plt

def init_weights(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        # To do: using torch.nn.init.kaiming_normal_() to initialize m.weight , choose the proper nonlinearity 
        # m.bias can be intialized to be zero, using torch.nn.init.zeros_()
        torch.nn.init.kaiming_normal_(m.weight, mode='fan_in',nonlinearity='relu')
        torch.nn.init.zeros_(m.bias)
    elif classname.find('Linear') != -1:
        # To do: Since we use the leakyReLU with slope of 0.2, you need to choose appropriate nonlinearity slope.
        # bias and weights need to be intialized respectively.
        torch.nn.init.kaiming_normal_(m.weight, mode='fan_in', a=0.2, nonlinearity='leaky_relu')
        torch.nn.init.zeros_(m.bias)

class QNetwork(nn.Module):    
    def __init__(self, input_shape, num_actions):
        super(QNetwork, self).__init__()
        self.input_shape = input_shape
        self.num_actions = num_actions

        # -----------------------------CHANGED: ----------------------------------
        # The convolutional part should have 3 convolutional layer, each followed by recified linear unit(RELU).
        # Conv1 : Kernel (8*8) ,no padding ,Stride :4, output channel: 32
        # Conv2 : Kernel (4*4) ,no padding ,Stride :2, output channel: 64
        # Conv3 : Kernel (3*3) ,no padding ,Stride :1, output channel: 64
        # Then followed by a fully connected layer self.fc1 , with 256 hidden nodes and leaklyReLU activation with the slope of 0.2.
        # The last output layer self.fc2 is also fully connected layer with the ouput equal to number of actions.
        # To prevent "dead neurons" of ReLU activation, you could use instead Leakyrelu with the negative slope of 0.2 after the fully-connected layer with 256 neurons.       
        # Hint: nn.sequential() , nn.Conv2d(), nn.Linear()....
        
        # To do: self.features only contains the 3 convolutional blocks and their nonliner activation functions # burada farklı olarak argümanlar yok
        self.features = nn.Sequential(
            nn.Conv2d(in_channels=input_shape[0], out_channels=32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),
            nn.ReLU()
        )
        # To do: apply the weight initialization scheme on self.features using the defined function init_weights.
        self.features.apply(init_weights)
        self.fc_input_dim = self.feature_size()
        # To do: Initialize fully connected layer () , then followed by leaklyReLU with slope 0.2, (We don't use ReLU here), and last linear layer
        # self.fc1 needs to go through weight initialization scheme.
        self.fc1 = nn.Linear(in_features=64*7*7, out_features=256) # buradaki input size ı değiştitp bi bak self.fc_input_dim 
        self.fc1.apply(init_weights)
        self.leaky_relu = nn.LeakyReLU(0.2) # negative_slope u kaldırabiliriz
        self.fc2 = nn.Linear(in_features=256, out_features=num_actions)
        
        
        
    def forward(self, state): # bu part totaly diff but it is okey for algo.
        # To do: before forwarding the state, the state should be normalized between [0,1], the original pixel range is [0,255]
        state=(state.float()/255.).cuda() # cuda yı bir check edelim !! fazladan birdaha cuda yapıyoruz
        feat=self.features(state)   
        feat = feat.view(feat.size(0), -1) # check x.size()[0] birde burada hep x kullanıyoruz o bir problem olabilr
        fc1=self.fc1(feat)
        lrelu=self.leaky_relu(fc1)
        x=self.fc2(lrelu).cuda()
        return x

    def feature_size(self):
        with torch.no_grad():
            return self.features(torch.zeros(1, *self.input_shape)).view(1, -1).size(1)

class LearningAgent: 
    def __init__(self, **kwargs):      
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print("Using {} for training the algorithm.".format(self.device))        
        self.env = kwargs.get('training_env',None)
        self.gamma = kwargs.get('gamma', 0.99)
        self.hard_update_interval = kwargs.get('target network update interval', 10000)
        
        self.update_step = 0
        
        # ---- initialize networks ----
        self.q_net1 = QNetwork(env.observation_space.shape, env.action_space.n).to(self.device)
        self.target_q_net1 = QNetwork(env.observation_space.shape, env.action_space.n).to(self.device)


        # To do: initialize optimizers using Adam optimizer, learning rate is in kwargs.
        self.q1_optimizer = optim.Adam(self.q_net1.parameters(), kwargs.get('learning rate', 1e-4))
        # To do: initialize the loss function (Huberloss) , keywords:  torch.nn.SmoothL1Loss , here we set the reduction scheme = 'mean'
        self.criterion = nn.SmoothL1Loss(reduction='mean')
        
        self.replay_buffer = ReplayBuffer(kwargs.get('memory size',1000000), env.observation_space.shape)
        


    def act(self, state):
        # To do: implement epsilon-greedy, where action is the index (just a number) , not a array/list/tensor.
        # Hint: to speed up, use with torch.no_grad():
        # In this function, convert the variable 'state' to a 4D tensor, which can be directly forwarded to network.
        state=torch.tensor(state).unsqueeze(0).to(self.device) 
        with torch.no_grad():
          a=self.q_net1.forward(state)
          if np.random.random() > self.epsilon:
            action=torch.argmax(a).item()#.cpu().detach().numpy() burada farklı olarak numpy kullanılmış
            #print(type(action))
          else:
            action = env.action_space.sample()  
        return action    



    def test_act(self, state):
        # To do: implement greedy policy, where action is the index (just a number) , not a array/list/tensor.
        # Hint: to speed up, use with torch.no_grad(): 
        # In this function, convert the variable 'state' to a 4D tensor, which can be directly forwarded to network.
        state=torch.tensor(state).unsqueeze(0).to(self.device) #hepsini within altına alabililriz hızlansın diye 
        with torch.no_grad():
          a=self.q_net1.forward(state)
          action=torch.argmax(a).item()
          #print(type(action))
        return action
    
   
    def update(self, batch_size):       
       # Very similar to previous DDQN, except for the loss definition and update manner of target network
        self.q_net1.train()
        
        transitions = self.replay_buffer.sample(batch_size)        
        states, actions, rewards, next_states, dones = transitions
        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)
        
        # To do: Implement DDQN here, should be similar/same as DDQN in previous task.
        
              
        #print("update")
        #print(type(next_states))
        # To do: Implement DDQN here, should be similar/same as DDQN in previous task.
        with torch.no_grad(): # check this part
          max_action_indexes = self.q_net1(next_states).argmax(1)#.detach().argmax(1)argmax prob olabilir .detach()
          labels_next = self.target_q_net1(next_states).gather(1, max_action_indexes.unsqueeze(1)).squeeze(1)
          labels = rewards + (self.gamma* labels_next*(1-dones)) # target q nets          
        predicted_targets = self.q_net1(states).gather(1,actions.unsqueeze(1)).squeeze(1) # current q values    
        # To do:  q loss using self.criterion
        q1_loss = self.criterion(input=predicted_targets,target=labels).to(self.device)   #target 32 input 32,1   
            

        # update q networks        
        self.q1_optimizer.zero_grad()
        q1_loss.backward()
        self.q1_optimizer.step()

        # --------To do: Perform Hard update of target networks every self.hard_update_interval-----------  #bunun bi etkisi var mı    
        if self.update_step % self.hard_update_interval == 0:    
          for target_param, param in zip(self.target_q_net1.parameters(), self.q_net1.parameters()):
            target_param.data.copy_(param.data)
        
        self.update_step += 1
        
        # just for monitoring Q loss, in case of wrong implementation, this could diverge to insane values.
        if self.update_step % 1000 == 999:
            print("Q loss: {}".format(q1_loss))

class ReplayBuffer(object):
    
    def __init__(self, max_size, state_dim):
        self.buffer = deque(maxlen=max_size)
        self.state_dim = state_dim
    
    def push(self, state, action, reward, next_state, done):
        experience = (state, action, reward, next_state, done)
        self.buffer.append(experience)

    
    def sample(self, batch_size):
        # To do: Same from previous task

        # first initialize using np.empty()
        state_batch = np.empty([batch_size,self.state_dim[0], self.state_dim[1], self.state_dim[2]])
        action_batch = np.empty([batch_size])
        reward_batch = np.empty([batch_size])
        next_state_batch = np.empty([batch_size,self.state_dim[0], self.state_dim[1], self.state_dim[2]])
        done_batch = np.empty([batch_size])
        # Take #batch_size samples from the replay buffer and write into different arrays.        
        batch = random.sample(range(len(self)),batch_size)
        # To Do here...
        for n in range(batch_size):
          (state, action, reward, next_state, done) = self.buffer[batch[n]]
          state_batch[n,:,:] = state
          action_batch[n] = action
          reward_batch[n] = reward
          next_state_batch[n,:,:] = next_state
          done_batch[n] = done
        #state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*random.sample(self.buffer, batch_size))          
        
        return state_batch, action_batch, reward_batch, next_state_batch, done_batch


    def __len__(self):
        return len(self.buffer)


def learn(agent, **kwargs):   
    # To do :You will save the statistics here
    update_step = 0
    train_start = kwargs.get('number of pre-interactions to start training', 50000)
    batch_size = kwargs.get('batch size', 32)
    try:
        max_steps = env._max_episode_steps# 
    except:
        max_steps = kwargs.get('max steps per episode', 27500)   
    max_episodes = kwargs.get('max episodes', 1000000)   
    update_every_n_step = kwargs.get('update every n steps', 4)   
    n_frame = 0
    running_100_mean = np.zeros(100)    
    epsilon_start = 1
    epsilon_final = 0.1

    
    epi_returns = [] # Important: you will save this statistics for plotting

    for episode in range(max_episodes):    
                
        state = env.reset()
        epi_return = 0
        for step in itertools.count():
            # Implement linearly-decaying epsilon
            agent.epsilon = max(epsilon_final, epsilon_start - (epsilon_start - epsilon_final)* n_frame/500000. )
            action= agent.act(state)                            
            next_state, reward, done, _ = env.step(action)            
            n_frame+=1        
            # Push the experience into replay buffer                                                              
            agent.replay_buffer.push(state, action, reward, next_state, done)
            epi_return += reward
            
            # To do: write an if condition that the training starts after the agent collects enough experience () and performs an update every 4 interactions.
            # Useful variable :  train_start , update_every_n_step , n_frame
            if (len(agent.replay_buffer) > train_start) and ((n_frame % update_every_n_step) == 0): 
                agent.update(batch_size)
                update_step += 1
            state = next_state
            
            if done == True or step >= max_steps:
                running_100_mean[episode%100] = epi_return
                epi_returns.append(epi_return)
                print('Episode : {} , episodic length : {}, return: {}, exp_count :{}, averged returnover past 100 episodes : {}'.format(episode , step, epi_return, n_frame, np.mean(running_100_mean[:min(100,episode+1)])))
                print("Number of frame: {}".format(n_frame))                 
                break

        #---------------To Do: Save the satistics of ----------------
        if episode%50 == 49: # you can change here
            epi_returns_np = np.array(epi_returns)
            # ---IMPORTNT: To do: save the statistics here for plotting ---
            person="RIC"
            np.save(person+"_DQN_atari_rewards",epi_returns_np)
        #----------------------------Testing-----------------------
        if episode%100 == 99:
            test_returns = np.zeros(15)
            print ('-----start Testing------')
            epi_return = 0       
  
            for i in range(15): # test for 15 episodes
                state = test_env.reset()            
                for t in range(max_steps):
                    # env.render()
                    action = agent.test_act(state)
                    next_state, reward, done, _ = test_env.step(action)
                    state = next_state
                    epi_return += reward                        
                    if done or (t == max_steps - 1):
                        test_returns[i] = epi_return
                        epi_return = 0
                        break                   
            print('Testing result, Returns: {}'.format(test_returns))

if __name__ == '__main__':  
    
    env_id = "PongNoFrameskip-v4" # you could change to other games, but only use 'NoFrameskip-v4' version. Some games e.g., spaceinvaders require changing the code in wrapper, i.e., frame_skip =3
    env = make_atari(env_id)
    env = wrap_deepmind(env)
    env = wrap_pytorch(env) # Note the env returns the unnormalized pixel [0,255].
    test_env = make_atari(env_id)
    test_env = wrap_deepmind(test_env, episode_life=False, clip_rewards=False)
    test_env = wrap_pytorch(test_env)
    
    # Hint: you could check the shape and content of the state, it is not yet normlized between [0,1]
    obs = env.reset()
    print(obs.shape)
    for i in range(obs.shape[0]):
        plt.imshow(obs[i])
    
    random_seed = None
    if random_seed:
        print("Random Seed: {}".format(random_seed))
        torch.manual_seed(random_seed)
        env.seed(random_seed)
        np.random.seed(random_seed)  
        random.seed(random_seed)
        
    # DQN params
    config = {}
    config.update({'gamma': 0.99})
    config.update({'learning rate': 1e-4})
    config.update({'memory size': 300000}) # 1M frame memory buffer takes around 8GB in RAM. For local computer, you could use 1M memory size. For Colab, size should be <= 0.3M.
    config.update({'target network update interval': 2500}) # every 10000 training steps, update the target model.
    config.update({'max episodes': 1000000}) # you don't need to run that long, but just keep running.
    config.update({'max steps per episode': 27500}) 
    config.update({'batch size': 32}) 
    config.update({'update every n steps': 4})
    config.update({'number of pre-interactions to start training': 10000})
    config.update({'training_env': env})
    config.update({'testing_env': test_env})
    #  agent
    agent = LearningAgent(**config)
    
    # train
    learn(agent, **config)